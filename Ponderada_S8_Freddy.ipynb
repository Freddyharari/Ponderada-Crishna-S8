{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU8HNpiuG8XI",
        "outputId": "71621c89-835d-436a-fada-bdc04f0b5dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "Keras: 3.10.0\n",
            "KerasNLP: 0.21.1\n",
            "KerasHub: 0.21.1\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U pip\n",
        "!pip -q install \"tensorflow==2.19.0\" \"tf-keras==2.19.0\" pandas scikit-learn matplotlib\n",
        "!pip -q install \"keras-nlp==0.17.0\" \"keras-hub==0.17.0\"\n",
        "\n",
        "import tensorflow as tf, keras, keras_nlp, keras_hub\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"Keras:\", keras.__version__)\n",
        "print(\"KerasNLP:\", keras_nlp.__version__)\n",
        "print(\"KerasHub:\", keras_hub.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install -U kaggle kagglehub\n",
        "\n",
        "import os, glob, zipfile, shutil, json, sys, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"data\"); data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "def list_csv():\n",
        "    files = sorted([p for p in data_dir.glob(\"**/*.csv\")],\n",
        "                   key=lambda p: p.stat().st_size if p.exists() else 0)\n",
        "    return files\n",
        "\n",
        "def have_csv():\n",
        "    files = list_csv()\n",
        "    if files:\n",
        "        print(\"CSV(s) em data/:\", [p.name for p in files])\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "if have_csv():\n",
        "    print(\"Pule para a Célula 3 (treino).\")\n",
        "else:\n",
        "\n",
        "    try:\n",
        "        import kagglehub\n",
        "        print(\"Tentando KaggleHub...\")\n",
        "        path = kagglehub.dataset_download(\"goyaladi/twitter-bot-detection-dataset\")\n",
        "        for p in Path(path).glob(\"*.csv\"):\n",
        "            shutil.copy(p, data_dir/p.name)\n",
        "        if have_csv():\n",
        "            print(\"Sucesso com KaggleHub. Pule para a Célula 3 (treino).\")\n",
        "    except Exception as e:\n",
        "        print(\"KaggleHub falhou:\", e)\n",
        "\n",
        "\n",
        "    if not have_csv():\n",
        "        try:\n",
        "            import kaggle\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"])\n",
        "            import kaggle\n",
        "\n",
        "        kaggle_json = Path(\"/root/.kaggle/kaggle.json\")\n",
        "        if not kaggle_json.exists():\n",
        "            from google.colab import files\n",
        "            print(\"📤 Envie o seu kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "            up = files.upload()\n",
        "            assert \"kaggle.json\" in up, \"Faltou o kaggle.json\"\n",
        "            kaggle_json.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with open(kaggle_json, \"wb\") as f: f.write(up[\"kaggle.json\"])\n",
        "            os.chmod(kaggle_json, 0o600)\n",
        "\n",
        "        print(\"➜ Tentando Kaggle CLI...\")\n",
        "        code = os.system(\"kaggle datasets download -d goyaladi/twitter-bot-detection-dataset -p data/ -unzip\")\n",
        "        if code != 0:\n",
        "            print(f\"⚠️ Kaggle CLI retornou código {code}. Provável termos não aceitos no site.\")\n",
        "        have_csv()\n",
        "\n",
        "    if not have_csv():\n",
        "        from google.colab import files\n",
        "        print(\"📤 Envie um CSV (ex.: bot_detection_data.csv) ou um ZIP contendo CSV(s).\")\n",
        "        up = files.upload()\n",
        "        for name, content in up.items():\n",
        "            fn = Path(name)\n",
        "            out = data_dir / fn.name\n",
        "            with open(out, \"wb\") as f:\n",
        "                f.write(content)\n",
        "            if fn.suffix.lower() == \".zip\":\n",
        "                try:\n",
        "                    with zipfile.ZipFile(out, \"r\") as z:\n",
        "                        z.extractall(data_dir)\n",
        "                except zipfile.BadZipFile:\n",
        "                    print(f\"ZIP inválido: {fn.name}\")\n",
        "        assert have_csv(), \"Não foi possível localizar CSV em data/ após o upload.\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhywidIiBNIs",
        "outputId": "1941cc85-3d08-4247-f453-730370aad31e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV(s) em data/: ['bot_detection_data.csv']\n",
            "Pule para a Célula 3 (treino).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, glob, re, json, numpy as np, pandas as pd, tensorflow as tf, keras, keras_nlp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "SEQ_LEN = 32\n",
        "BATCH   = 2\n",
        "MAX_N   = 3000\n",
        "CAP_TRAIN_STEPS = 40\n",
        "CAP_VAL_STEPS   = 10\n",
        "EPOCHS          = 2\n",
        "\n",
        "\n",
        "csvs = sorted(glob.glob(\"data/*.csv\"), key=os.path.getsize)\n",
        "assert csvs, \"Nenhum CSV em data/. Rode a célula A2 para baixar/subir o dataset.\"\n",
        "csv_path = csvs[-1]\n",
        "print(\"CSV:\", os.path.basename(csv_path))\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "text_candidates  = [\"Tweet\",\"tweet\",\"text\",\"Text\",\"content\",\"description\",\"bio\"]\n",
        "label_candidates = [\"Bot Label\",\"bot\",\"label\",\"target\",\"bot_label\",\"is_bot\"]\n",
        "text_col  = next((c for c in text_candidates  if c in df.columns), df.columns[0])\n",
        "label_col = next((c for c in label_candidates if c in df.columns), df.columns[-1])\n",
        "print(\"cols -> text:\", text_col, \"| label:\", label_col)\n",
        "\n",
        "def clean(s:str)->str:\n",
        "    s = str(s)\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\",\" \", s)\n",
        "    s = re.sub(r\"@\\w+\",\"@user\", s)\n",
        "    s = re.sub(r\"#(\\w+)\", r\"\\1\", s)\n",
        "    return \" \".join(s.split())\n",
        "def to01(x)->int:\n",
        "    s = str(x).strip().lower()\n",
        "    return 1 if (s in {\"1\",\"true\",\"bot\",\"yes\",\"y\"} or \"bot\" in s) else 0\n",
        "\n",
        "df = df[[text_col,label_col]].dropna()\n",
        "X = df[text_col].astype(str).apply(clean).tolist()\n",
        "y = df[label_col].apply(to01).astype(int).tolist()\n",
        "\n",
        "\n",
        "if MAX_N and len(X) > MAX_N:\n",
        "    X, y = X[:MAX_N], y[:MAX_N]\n",
        "print(f\"tamanho do conjunto usado: {len(X)}\")\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "preproc = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_base_en_uncased\",\n",
        "    sequence_length=SEQ_LEN\n",
        ")\n",
        "\n",
        "mini_backbone = keras_nlp.models.BertBackbone(\n",
        "    vocabulary_size=preproc.tokenizer.vocabulary_size(),\n",
        "    num_layers=2,\n",
        "    num_heads=2,\n",
        "    hidden_dim=128,\n",
        "    intermediate_dim=256,\n",
        "    max_sequence_length=SEQ_LEN,\n",
        ")\n",
        "\n",
        "model = keras_nlp.models.BertClassifier(\n",
        "    backbone=mini_backbone,\n",
        "    preprocessor=preproc,\n",
        "    num_classes=2,\n",
        ")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices((tf.constant(Xtr), tf.constant(np.array(ytr, dtype=np.int64))))\n",
        "            .shuffle(min(2048, len(Xtr))).batch(BATCH).prefetch(AUTOTUNE))\n",
        "val_ds   = (tf.data.Dataset.from_tensor_slices((tf.constant(Xte), tf.constant(np.array(yte, dtype=np.int64))))\n",
        "            .batch(BATCH).prefetch(AUTOTUNE))\n",
        "\n",
        "\n",
        "train_ds_fast = train_ds.take(CAP_TRAIN_STEPS)\n",
        "val_ds_fast   = val_ds.take(CAP_VAL_STEPS)\n",
        "\n",
        "\n",
        "opt     = keras.optimizers.Adam(2e-4)\n",
        "loss    = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
        "\n",
        "model.compile(optimizer=opt, loss=loss, metrics=metrics, run_eagerly=True, jit_compile=False)\n",
        "print(\"Treinando (leve e capado)…\")\n",
        "history = model.fit(train_ds_fast, validation_data=val_ds_fast, epochs=EPOCHS, verbose=2)\n",
        "\n",
        "\n",
        "logits = model.predict(val_ds_fast, verbose=0)\n",
        "yp = np.argmax(logits, axis=1)\n",
        "yref = np.concatenate([b.numpy() for _, b in val_ds_fast], axis=0)\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "acc  = accuracy_score(yref, yp)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(yref, yp, average=\"binary\", pos_label=1, zero_division=0)\n",
        "print(f\"VAL(capado)  acc={acc:.4f}  prec={prec:.4f}  rec={rec:.4f}  f1={f1:.4f}\")\n",
        "\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "model.save(\"artifacts/model_keras.keras\")\n",
        "\n",
        "\n",
        "with open(\"artifacts/tokenizer_info.json\",\"w\") as f:\n",
        "    json.dump({\"preset_vocab\": \"bert_base_en_uncased\", \"sequence_length\": SEQ_LEN,\n",
        "               \"backbone_config\": {\"layers\":2,\"heads\":2,\"hidden\":128,\"intermediate\":256}}, f, indent=2)\n",
        "with open(\"artifacts/label_mapping.json\",\"w\") as f:\n",
        "    json.dump({\"id2label\": {0:\"human\", 1:\"bot\"}}, f, indent=2)\n",
        "with open(\"artifacts/metrics_test.json\",\"w\") as f:\n",
        "    json.dump({\"accuracy\":float(acc),\"precision\":float(prec),\"recall\":float(rec),\"f1\":float(f1)}, f, indent=2)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(yref, yp, labels=[0,1])\n",
        "plt.figure(figsize=(3,3)); plt.imshow(cm); plt.title(\"Confusão (val capado)\")\n",
        "plt.xticks([0,1], [\"human\",\"bot\"]); plt.yticks([0,1], [\"human\",\"bot\"])\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\")\n",
        "plt.tight_layout(); plt.savefig(\"artifacts/confusion_matrix.png\"); plt.close()\n",
        "\n",
        "print(\"Entregáveis em artifacts/: model_keras/, tokenizer_info.json, label_mapping.json, metrics_test.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0XJUDlNBVks",
        "outputId": "18c8599d-3533-4adf-8229-6435b16dacb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV: bot_detection_data.csv\n",
            "cols -> text: Tweet | label: Bot Label\n",
            "tamanho do conjunto usado: 3000\n",
            "Treinando (leve e capado)…\n",
            "Epoch 1/2\n",
            "40/40 - 25s - 620ms/step - acc: 0.5375 - loss: 0.7065 - val_acc: 0.7000 - val_loss: 0.6577\n",
            "Epoch 2/2\n",
            "40/40 - 38s - 960ms/step - acc: 0.5250 - loss: 0.6955 - val_acc: 0.3500 - val_loss: 0.6933\n",
            "VAL(capado)  acc=0.3500  prec=1.0000  rec=0.0714  f1=0.1333\n",
            "Entregáveis em artifacts/: model_keras/, tokenizer_info.json, label_mapping.json, metrics_test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, json, shutil, datetime\n",
        "\n",
        "\n",
        "try:\n",
        "    _ = (model, acc, prec, rec, f1, SEQ_LEN)\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\"Rode a célula de TREINO antes desta (model/metrics não estão na memória).\") from e\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "\n",
        "model_path = \"artifacts/model_keras.keras\"\n",
        "model.save(model_path)\n",
        "print(\"✔ Modelo salvo em:\", model_path)\n",
        "\n",
        "\n",
        "try:\n",
        "    model.export(\"artifacts/model_savedmodel\")\n",
        "    saved_export = True\n",
        "    print(\"SavedModel exportado em artifacts/model_savedmodel/\")\n",
        "except Exception as e:\n",
        "    saved_export = False\n",
        "    print(\"Export SavedModel (opcional) não feita:\", e)\n",
        "\n",
        "\n",
        "with open(\"artifacts/tokenizer_info.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"preset_vocab\": \"bert_base_en_uncased\",\n",
        "        \"sequence_length\": int(SEQ_LEN),\n",
        "        \"backbone_config\": {\"layers\":2,\"heads\":2,\"hidden\":128,\"intermediate\":256}\n",
        "    }, f, indent=2)\n",
        "\n",
        "with open(\"artifacts/label_mapping.json\",\"w\") as f:\n",
        "    json.dump({\"id2label\": {0:\"human\", 1:\"bot\"}}, f, indent=2)\n",
        "\n",
        "with open(\"artifacts/metrics_test.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"f1\": float(f1)\n",
        "    }, f, indent=2)\n",
        "\n",
        "#  Relatório\n",
        "hoje = datetime.date.today().isoformat()\n",
        "lines = []\n",
        "lines += [\n",
        "\"# Detecção de Bots no Twitter com BERT (Keras)\",\n",
        "f\"**Data:** {hoje}\",\n",
        "\"\",\n",
        "\"## 1. Objetivo\",\n",
        "\"Treinar uma rede neural **BERT** no **Keras** para detectar bots no Twitter utilizando o dataset **Twitter-Bot Detection** (Kaggle).\",\n",
        "\"\",\n",
        "\"## 2. Dados (Kaggle)\",\n",
        "\"- Conjunto: *Twitter-Bot Detection Dataset*.\",\n",
        "\"- Colunas detectadas no CSV: texto = `Tweet` | rótulo = `Bot Label`.\",\n",
        "\"- Rotulagem binária mapeada para 0: human, 1: bot.\",\n",
        "\"\",\n",
        "\"## 3. Preparação\",\n",
        "\"- Limpezas: remoção de URLs, normalização de menções para `@user`, remoção de `#` mantendo a palavra.\",\n",
        "\"- Divisão: treino 80% / teste 20% (stratify).\",\n",
        "f\"- Tamanho de sequência para o modelo: **{int(SEQ_LEN)}** tokens.\",\n",
        "\"\",\n",
        "\"## 4. Modelo\",\n",
        "\"- **BERT (KerasNLP)** *mini* treinado do zero (leve para Colab):\",\n",
        "\"  - camadas = 2, cabeças = 2, dimensão oculta = 128, intermediária = 256.\",\n",
        "\"- Perda: `SparseCategoricalCrossentropy(from_logits=True)`.\",\n",
        "\"- Otimizador: Adam.\",\n",
        "\"- Métricas: Acurácia + Precisão/Recall/F1 (classe bot).\",\n",
        "\"\",\n",
        "\"## 5. Treinamento\",\n",
        "\"- Subamostragem do dataset para caber na RAM.\",\n",
        "\"- Limite de passos por época (CAP_TRAIN_STEPS/CAP_VAL_STEPS).\",\n",
        "\"- Épocas: 2.\",\n",
        "\"\",\n",
        "\"## 6. Resultados (val capado)\",\n",
        "f\"- **Accuracy:** {acc:.4f}\",\n",
        "f\"- **Precision (bot):** {prec:.4f}\",\n",
        "f\"- **Recall (bot):** {rec:.4f}\",\n",
        "f\"- **F1 (bot):** {f1:.4f}\",\n",
        "\"\",\n",
        "\"## 7. Discussão\",\n",
        "\"- Desempenho condizente com BERT pequeno sem pré-treino e orçamento de passos reduzido.\",\n",
        "\"- Melhorias possíveis: usar preset pré-treinado, mais épocas, ajuste de `sequence_length`/`batch`, class weights, e pequenas augments de texto.\",\n",
        "\"\",\n",
        "\"## 8. Reprodutibilidade\",\n",
        "\"1) Instalar dependências (TF 2.19, Keras 3, KerasNLP).\",\n",
        "\"2) Colocar o CSV do Kaggle em `data/`.\",\n",
        "\"3) Executar a célula de treino (mini-BERT).\",\n",
        "\"4) Executar esta célula de salvamento/relatório.\",\n",
        "\"\",\n",
        "\"## 9. Como carregar o modelo para inferência\",\n",
        "\"```python\",\n",
        "\"import keras, numpy as np\",\n",
        "'m = keras.models.load_model(\"artifacts/model_keras.keras\")',\n",
        "'logits = m.predict([\"This account tweets the same link every hour\"])',\n",
        "\"pred = int(np.argmax(logits, axis=1)[0])\",\n",
        "'print(\"bot\" if pred==1 else \"human\")',\n",
        "\"```\",\n",
        "\"\",\n",
        "\"## 10. Arquivos Entregues\",\n",
        "\"- `artifacts/model_keras.keras` (modelo treinado)\",\n",
        "\"- `artifacts/tokenizer_info.json` (config de pré-processamento/seq_len)\",\n",
        "\"- `artifacts/label_mapping.json`\",\n",
        "\"- `artifacts/metrics_test.json`\",\n",
        "\"- `artifacts/Relatorio_Projeto.md`\",\n",
        "]\n",
        "with open(\"artifacts/Relatorio_Projeto.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "\n",
        "zip_path = shutil.make_archive(\"ENTREGA_FINAL\", \"zip\", \"artifacts\")\n",
        "print(\"ENTREGA_FINAL gerado:\", zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbNktduXdaqF",
        "outputId": "b6e449af-038b-4146-cc30-66bf0852696b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Modelo salvo em: artifacts/model_keras.keras\n",
            "Saved artifact at 'artifacts/model_savedmodel'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): Dict[['token_ids', TensorSpec(shape=(None, None), dtype=tf.int32, name='token_ids')], ['segment_ids', TensorSpec(shape=(None, None), dtype=tf.int32, name='segment_ids')], ['padding_mask', TensorSpec(shape=(None, None), dtype=tf.int32, name='padding_mask')]]\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138323966703184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966699152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966691472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966704528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966692048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966690896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966691280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966690704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966690320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966691088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966690512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966693584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966693968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966694352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966692240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966690128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966695312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966691664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966693392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966693776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966689552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966694544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966696656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966694928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966692624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966689936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966689360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966691856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966695888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966696848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966705296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966699728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966698960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966702800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966703760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966693008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966702224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966703568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138323966699536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138324112373904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138324112372176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "SavedModel exportado em artifacts/model_savedmodel/\n",
            "ENTREGA_FINAL gerado: /content/ENTREGA_FINAL.zip\n"
          ]
        }
      ]
    }
  ]
}